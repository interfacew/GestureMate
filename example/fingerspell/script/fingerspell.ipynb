{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ced9ae2-0a48-4059-abdf-2ba1d642a36a",
   "metadata": {},
   "source": [
    "### 使用holistic获取人体关键点位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1ffc20-6820-42ef-8c5e-b02b2ec3aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import mediapipe.python.solutions as sol\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    sol.drawing_utils.draw_landmarks(image, results.face_landmarks, sol.holistic.FACEMESH_TESSELATION,\n",
    "                              landmark_drawing_spec=None,\n",
    "                              connection_drawing_spec=sol.drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "    sol.drawing_utils.draw_landmarks(image, results.face_landmarks, sol.holistic.FACEMESH_CONTOURS,\n",
    "                              landmark_drawing_spec=None,\n",
    "                              connection_drawing_spec=sol.drawing_styles.get_default_face_mesh_contours_style()) \n",
    "    # Draw pose connections\n",
    "    sol.drawing_utils.draw_landmarks(image, results.pose_landmarks, sol.holistic.POSE_CONNECTIONS,\n",
    "                              sol.drawing_styles.get_default_pose_landmarks_style()) \n",
    "    # Draw left hand connections\n",
    "    sol.drawing_utils.draw_landmarks(image, results.left_hand_landmarks, sol.holistic.HAND_CONNECTIONS,\n",
    "                             sol.drawing_styles.get_default_hand_landmarks_style(),sol.drawing_styles.get_default_hand_connections_style()) \n",
    "    # Draw right hand connections  \n",
    "    sol.drawing_utils.draw_landmarks(image, results.right_hand_landmarks, sol.holistic.HAND_CONNECTIONS,\n",
    "                              sol.drawing_styles.get_default_hand_landmarks_style(),sol.drawing_styles.get_default_hand_connections_style()) \n",
    "def extract_landmarks(x):\n",
    "    result=[]\n",
    "    if not x.pose_landmarks is None:\n",
    "        a=x.pose_landmarks.landmark\n",
    "        for i in range(len(a)):\n",
    "            result.append([a[i].x,a[i].y,a[i].z])\n",
    "    else:\n",
    "        result+=[[0,0,0]]*33\n",
    "    if not x.left_hand_landmarks is None:\n",
    "        a=x.left_hand_landmarks.landmark\n",
    "        for i in range(len(a)):\n",
    "            result.append([a[i].x,a[i].y,a[i].z])\n",
    "    else:\n",
    "        result+=[[0,0,0]]*21\n",
    "    if not x.right_hand_landmarks is None:\n",
    "        a=x.right_hand_landmarks.landmark\n",
    "        for i in range(len(a)):\n",
    "            result.append([a[i].x,a[i].y,a[i].z])\n",
    "    else:\n",
    "        result+=[[0,0,0]]*21\n",
    "    if not x.face_landmarks is None:\n",
    "        a=x.face_landmarks.landmark\n",
    "        for i in range(len(a)):\n",
    "            result.append([a[i].x,a[i].y,a[i].z])\n",
    "    else:\n",
    "        result+=[[0,0,0]]*468\n",
    "    assert len(result)==543\n",
    "    return result\n",
    "    \n",
    "def start_listen(detect):\n",
    "    camera=cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
    "    camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "    camera.set(cv2.CAP_PROP_FPS,60)\n",
    "    with sol.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic:\n",
    "        seq=[]\n",
    "        while camera.isOpened():\n",
    "            ret, frame = camera.read()\n",
    "            clear_output(wait=True)\n",
    "            frame=frame[:,::-1,:]\n",
    "            \n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "            image.flags.writeable = False                  # Image is no longer writeable\n",
    "            results = holistic.process(image)                 # Make prediction\n",
    "            image.flags.writeable = True                   # Image is now writeable \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    \n",
    "            draw_styled_landmarks(image, results)\n",
    "            image=detect(image,extract_landmarks(results))\n",
    "            \n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "    \n",
    "            if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "                break\n",
    "        camera.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe168bf-cc1c-4b5d-906a-1ab88898a0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e154c32-5f1d-4aae-af20-35c13f9258bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'space']\n"
     ]
    }
   ],
   "source": [
    "# train_dir=\"D:\\\\sjtu\\\\project\\\\asl_alphabet_train\\\\asl_alphabet_train\"\n",
    "train_dir=\"D:\\\\sjtu\\\\project\\\\dataset\\\\data\\\\\"\n",
    "# test_dir=\"D:\\\\sjtu\\\\project\\\\asl_alphabet_test\\\\asl_alphabet_test\"\n",
    "train_detect=\"D:\\\\sjtu\\\\project\\\\dataset\\\\data_detect\\\\\"\n",
    "# train_detect=\"D:\\\\sjtu\\\\project\\\\asl_alphabet_train\\\\asl_alphabet_train_detect\"\n",
    "token_list=[]\n",
    "\n",
    "import os\n",
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "# from mediapipe.tasks import python\n",
    "# from mediapipe.tasks.python import vision\n",
    "from tqdm import tqdm\n",
    "\n",
    "HandLandmarkerResult = mp.tasks.vision.HandLandmarkerResult\n",
    "hand_path='D:\\\\sjtu\\\\project\\\\hand_landmarker.task'\n",
    "hand_file=open(hand_path,\"rb\")\n",
    "hand_data=hand_file.read()\n",
    "hand_file.close()\n",
    "base_options=mp.tasks.BaseOptions(model_asset_buffer=hand_data)\n",
    "options=mp.tasks.vision.HandLandmarkerOptions(base_options=base_options,num_hands=1)\n",
    "hand_detector=mp.tasks.vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "def draw_hand_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "    # Loop through the detected hands to visualize.\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        handedness = handedness_list[idx]\n",
    "        # Draw the hand landmarks.\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "        annotated_image,\n",
    "        hand_landmarks_proto,\n",
    "        solutions.hands.HAND_CONNECTIONS,\n",
    "        solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "        solutions.drawing_styles.get_default_hand_connections_style())\n",
    "        # Get the top left corner of the detected hand's bounding box.\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "        # Draw handedness (left or right hand) on the image.\n",
    "        cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "    return annotated_image\n",
    "if not os.path.exists(train_detect):\n",
    "    os.mkdir(train_detect)\n",
    "token_list=sorted(os.listdir(train_dir))\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa17fc8-cd3c-4069-a510-5c0d84f1038d",
   "metadata": {},
   "source": [
    "### 数据集收集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "460a4634-526d-4a72-ba52-9ecb76a55a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(token_list)):\n",
    "    token=token_list[i]\n",
    "    camera=cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
    "    camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "    camera.set(cv2.CAP_PROP_FPS,60)\n",
    "    cnt=0\n",
    "    data=[]\n",
    "    with sol.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic:\n",
    "        seq=[]\n",
    "        while camera.isOpened():\n",
    "            ret, frame = camera.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            clear_output(wait=True)\n",
    "            frame=frame[:,::-1,:]\n",
    "            \n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "            image.flags.writeable = False                  # Image is no longer writeable\n",
    "            results = holistic.process(image)                 # Make prediction\n",
    "            image.flags.writeable = True                   # Image is now writeable \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "            cv.putText(image,f\"Token: {token} cnt {cnt}\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "            \n",
    "            draw_styled_landmarks(image, results)\n",
    "            res=extract_landmarks(results)\n",
    "            if res[33][0]!=0:\n",
    "                res=res[33:54]\n",
    "            else:\n",
    "                res=res[54:75]                \n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "            a=cv2.waitKey(20)\n",
    "            if a & 0xFF == ord('q'):\n",
    "                break\n",
    "            if a & 0xFF == ord(' '):\n",
    "                cnt+=1\n",
    "                data.append([i,res])\n",
    "                cv2.waitKey(100)\n",
    "                if cnt>=100:\n",
    "                    break\n",
    "        camera.release()\n",
    "        cv2.destroyAllWindows()\n",
    "with open(os.path.join(train_dir,\"./../datas_diy.json\"),\"w\") as f:\n",
    "    f.write(str(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb8503-a3c7-41d4-9b20-75d60b0862ad",
   "metadata": {},
   "source": [
    "### 处理train数据  \n",
    "1. 获取图片\n",
    "2. 识别位点位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40c88bb-3655-4a8b-b211-17e09fa77f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/4000 [00:00<?, ?it/s]D:\\Programs\\anaconda3\\envs\\sign\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "  7%|█████▋                                                                         | 290/4000 [00:22<04:41, 13.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m cnt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(files):\n\u001b[1;32m---> 11\u001b[0m     image\u001b[38;5;241m=\u001b[39mcv\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path,file))\n\u001b[0;32m     12\u001b[0m     mp_image\u001b[38;5;241m=\u001b[39mmp\u001b[38;5;241m.\u001b[39mImage(image_format\u001b[38;5;241m=\u001b[39mmp\u001b[38;5;241m.\u001b[39mImageFormat\u001b[38;5;241m.\u001b[39mSRGB,data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(cv\u001b[38;5;241m.\u001b[39mcvtColor(image,cv\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)))\n\u001b[0;32m     13\u001b[0m     result\u001b[38;5;241m=\u001b[39mhand_detector\u001b[38;5;241m.\u001b[39mdetect(mp_image)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datas=[]\n",
    "for i in range(len(token_list)):\n",
    "    token=token_list[i]\n",
    "    path=os.path.join(train_dir,token)\n",
    "    detect_path=os.path.join(train_detect,token)\n",
    "    if not os.path.exists(detect_path):\n",
    "        os.mkdir(detect_path)\n",
    "    files=os.listdir(path)\n",
    "    cnt=0\n",
    "    for file in tqdm(files):\n",
    "        image=cv.imread(os.path.join(path,file))\n",
    "        mp_image=mp.Image(image_format=mp.ImageFormat.SRGB,data=np.array(cv.cvtColor(image,cv.COLOR_BGR2RGB)))\n",
    "        result=hand_detector.detect(mp_image)\n",
    "        if len(result.hand_landmarks)==0:\n",
    "            continue\n",
    "        points=[]\n",
    "        for j in range(21):\n",
    "            points.append([result.hand_landmarks[0][j].x,result.hand_landmarks[0][j].y,result.hand_landmarks[0][j].z])\n",
    "        datas.append([i,points])\n",
    "        image=draw_hand_landmarks_on_image(image,result)\n",
    "        cv.imwrite(os.path.join(detect_path,file),image)\n",
    "        cnt+=1\n",
    "    print(f\"image for {token}: total {len(files)}, detect {cnt}\")\n",
    "# print(datas)\n",
    "with open(os.path.join(train_dir,\"./../datas_3d.json\"),\"w\") as f:\n",
    "    f.write(str(datas))\n",
    "train_datas=datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961a6cb-6ab8-4a64-a521-c632a7f8b42d",
   "metadata": {},
   "source": [
    "### 处理test数据\n",
    "@Deprecated 现在test由train取样获得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8448766-dba5-4c71-8e19-1882706812d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\anaconda3\\envs\\sign\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test total 28, detect 14\n"
     ]
    }
   ],
   "source": [
    "datas=[]\n",
    "files=os.listdir(test_dir)\n",
    "cnt=0\n",
    "for file in files:\n",
    "    image=cv.imread(os.path.join(test_dir,file))\n",
    "    mp_image=mp.Image(image_format=mp.ImageFormat.SRGB,data=np.array(cv.cvtColor(image,cv.COLOR_BGR2RGB)))\n",
    "    result=hand_detector.detect(mp_image)\n",
    "    if len(result.hand_landmarks)==0:\n",
    "        continue\n",
    "    points=[]\n",
    "    for j in range(21):\n",
    "        points.append([result.hand_landmarks[0][j].x,result.hand_landmarks[0][j].y,result.hand_landmarks[0][j].z])\n",
    "    datas.append([i,points])\n",
    "    image=draw_hand_landmarks_on_image(image,result)\n",
    "    cv.imshow(f\"{file.split('_')[0]}\",image)\n",
    "    cv.waitKey(2000)\n",
    "    cv.destroyAllWindows()\n",
    "    cnt+=1\n",
    "print(f\"test total 28, detect {cnt}\")\n",
    "test_datas=datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870932f-a0f8-4919-8a07-4227ff480156",
   "metadata": {},
   "source": [
    "### 读取数据并准备DataLoader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5520dd7c-d01b-4442-9e4f-6db33890c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 86785/86785 [00:05<00:00, 15774.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train data 85917(1343 batch)\n",
      "Loaded test data 867(14 batch)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "train_data=[]\n",
    "with open(os.path.join(train_dir,\"./../datas.json\"),\"r\") as f:\n",
    "    train_data=json.loads(f.read())\n",
    "\n",
    "def normalize(points):\n",
    "    res=[]\n",
    "    maxx,maxy,minx,miny=-1,-1,1,1\n",
    "    maxz,minz=-1,1\n",
    "    for point in points:\n",
    "        maxx=max(maxx,point[0])\n",
    "        maxy=max(maxy,point[1])\n",
    "        minx=min(minx,point[0])\n",
    "        miny=min(miny,point[1])\n",
    "        maxz=max(maxz,point[2])\n",
    "        minz=min(minz,point[2])\n",
    "    max_delta=max(maxx-minx,maxy-miny)\n",
    "    if max_delta<=1e-5:\n",
    "        return [[0,0,0]]*21\n",
    "    for point in points:\n",
    "        newx=(point[0]-(maxx+minx)/2)/max_delta\n",
    "        newy=(point[1]-(maxy+miny)/2)/max_delta\n",
    "        newz=(point[2]-(maxz+minz)/2)/(maxz-minz)\n",
    "        if newx<-0.5-1e-6 or newx>0.5+1e-6 or newy<-0.5-1e-6 or newy>0.5+1e-6 or newz<-0.5-1e-6 or newz>0.5+1e-6:\n",
    "            print([point[0],point[1],newx,newy,max_delta,maxx,maxy,minx,miny])\n",
    "            raise ValueError\n",
    "        res.append([newx,newy,newz])\n",
    "    return res\n",
    "train_data=[[x[0],normalize(x[1])] for x in tqdm(train_data)]\n",
    "# print(train_data[0])\n",
    "\n",
    "def rotate_points(points):\n",
    "    output_rotated_points=[]\n",
    "    angle=random.randint(-15,15)\n",
    "    rad_angle=np.deg2rad(angle)\n",
    "    rotation_matrix = np.array([[np.cos(rad_angle), -np.sin(rad_angle),0],\n",
    "                                [np.sin(rad_angle), np.cos(rad_angle),0],\n",
    "                               [0,0,1]])\n",
    "    for point in points:\n",
    "        rotated_points = np.dot(point, rotation_matrix)\n",
    "        output_rotated_points.append(rotated_points.tolist())\n",
    "    return output_rotated_points\n",
    "\n",
    "def modify(x):\n",
    "    normalize(rotate_points(np.array(x)))\n",
    "    if random.randint(0,1)==0:\n",
    "        x=[[-i[0],-i[1],i[2]] for i in x]\n",
    "    return x\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,x):\n",
    "        self.data=x\n",
    "    def __getitem__(self,x):\n",
    "        label=self.data[x][0]\n",
    "        points=np.array(self.data[x][1])\n",
    "        return label,points\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "train_data=train_data\n",
    "random.shuffle(train_data)\n",
    "test_data=train_data[:len(train_data)//100]\n",
    "train_data=train_data[len(train_data)//100+1:]\n",
    "\n",
    "train_dataset=myDataset(train_data)\n",
    "train_loader=DataLoader(train_dataset,batch_size=64)\n",
    "print(f\"Loaded train data {len(train_dataset)}({len(train_loader)} batch)\")\n",
    "\n",
    "test_dataset=myDataset(test_data)\n",
    "test_loader=DataLoader(test_dataset,batch_size=64)\n",
    "print(f\"Loaded test data {len(test_dataset)}({len(test_loader)} batch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0434b-f88a-4851-810e-e42ef8950ef3",
   "metadata": {},
   "source": [
    "### 设计并训练模型   \n",
    "手部点位总计21个，将其映射到~~29~~28个点作为模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5aabb6-7472-4453-90b0-b70439d2e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SignClassifier,self).__init__()\n",
    "        self.ff1=nn.Linear(21*3,1024)\n",
    "        self.ff2=nn.Linear(1024,512)\n",
    "        # self.ff3=nn.Linear(512,29)\n",
    "        self.ff3=nn.Linear(512,28)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "    def forward(self,x):\n",
    "        x=self.flatten(x)\n",
    "        x=self.dropout1(self.relu(self.ff1(x)))\n",
    "        x=self.dropout2(self.relu(self.ff2(x)))\n",
    "        return self.ff3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ff40dd-1a9d-48b1-80b7-aaf73ca624ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.337205: 100%|██████████████████████████████████████████████████████████████| 1343/1343 [00:20<00:00, 66.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 257.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.5110419669321605 acc 0.8719723183391004\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.218919: 100%|██████████████████████████████████████████████████████████████| 1343/1343 [00:20<00:00, 64.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 252.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.30631272388356073 acc 0.9284890426758939\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.118539: 100%|██████████████████████████████████████████████████████████████| 1343/1343 [00:22<00:00, 59.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 305.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.23159179144671985 acc 0.9504036908881199\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.083978: 100%|██████████████████████████████████████████████████████████████| 1343/1343 [00:21<00:00, 63.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 335.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.19141275488904544 acc 0.9550173010380623\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.092011: 100%|██████████████████████████████████████████████████████████████| 1343/1343 [00:22<00:00, 58.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 217.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.16612651944160461 acc 0.9573241061130334\n"
     ]
    }
   ],
   "source": [
    "def train(dataloader,model,loss_fn,optimizer):\n",
    "    model.train()\n",
    "    pbar=tqdm(dataloader)\n",
    "    for y,X in pbar:\n",
    "        # print(X.shape)\n",
    "        X=[modify(i) for i in X.tolist()]\n",
    "        X=torch.Tensor(X).to(device)\n",
    "        # print(X.shape)\n",
    "        # print(X)\n",
    "        y=torch.Tensor(y).to(device)\n",
    "        # print(y.shape)\n",
    "        # break\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred,y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        pbar.set_description(f\"loss: {loss.item():.6f}\")\n",
    "def test(dataloader,model,loss_fn):\n",
    "    model.eval()\n",
    "    pbar=tqdm(dataloader)\n",
    "    losssum=0\n",
    "    cnt=0\n",
    "    acc=0\n",
    "    for y,X in pbar:\n",
    "        X=torch.Tensor(X).type(torch.float).to(device)\n",
    "        # print(X.shape)\n",
    "        y=torch.Tensor(y).to(device)\n",
    "        # print(y.shape)\n",
    "        # break\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred,y)\n",
    "        acc+=(pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        losssum+=loss.item()\n",
    "        cnt+=1\n",
    "    print(f\"loss {losssum/cnt} acc {acc/len(dataloader.dataset)}\")\n",
    "        \n",
    "device='cuda'\n",
    "model=SignClassifier()\n",
    "model.to(device)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "epoch=5\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(f\"Epoch {i+1}\")\n",
    "    train(train_loader,model,loss_fn,optimizer)  \n",
    "    test(test_loader,model,loss_fn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152b4ebc-f6be-484c-800f-93ff4d7178ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'PointDetect_3d.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a02d87-297d-4acf-95da-cd507e2f1670",
   "metadata": {},
   "source": [
    "### 实时检测\n",
    "通过mediapipe获取到关键点位置之后传入模型进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42e2f61a-839b-42d0-8ddc-fe972877d74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]] [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
      "[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]] [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
      "torch.Size([2, 21, 3])\n",
      "0.00219 0.00376 0.00250 0.09469 0.00521 0.02640 0.02697 0.02377 0.14398 0.05386 0.06207 0.04484 0.00922 0.02222 0.00345 0.02066 0.01744 0.04630 0.00320 0.00354 0.17553 0.04375 0.02067 0.02743 0.01144 0.08026 0.00653 0.01813 \n",
      "0.00219 0.00376 0.00250 0.09469 0.00521 0.02640 0.02697 0.02377 0.14398 0.05386 0.06207 0.04484 0.00922 0.02222 0.00345 0.02066 0.01744 0.04630 0.00320 0.00354 0.17553 0.04375 0.02067 0.02743 0.01144 0.08026 0.00653 0.01813 \n"
     ]
    }
   ],
   "source": [
    "model=torch.load('PointDetect_3d.pth')\n",
    "device='cuda'\n",
    "import math\n",
    "from datetime import datetime\n",
    "start_time=0\n",
    "last_token=\"@\"\n",
    "sentence=\"@\"\n",
    "ACTIVATE_RATE=60/100\n",
    "def detection(image,x):\n",
    "    global last_token,start_time,sentence,ACTIVATE_RATE\n",
    "    a=normalize(x[33:54])\n",
    "    b=normalize(x[54:75])\n",
    "    print(x[33:54],a)\n",
    "    print(x[54:75],b)\n",
    "    data=torch.Tensor([a,b]).to(device)\n",
    "    # data=torch.Tensor([normalize(x)]).to(device)\n",
    "    # print(data)\n",
    "    print(data.shape)\n",
    "    res=F.softmax(model(data))\n",
    "    # print(F.softmax(res).tolist())\n",
    "    for i in res[0]:\n",
    "        print(f\"{i:>.5f}\",end=\" \")\n",
    "    print()\n",
    "    for i in res[1]:\n",
    "        print(f\"{i:>.5f}\",end=\" \")\n",
    "    print()\n",
    "    a=res.argmax(1)\n",
    "    for i in range(len(token_list)):\n",
    "        if x[54][0]!=0:\n",
    "            cv.rectangle(image,(0,25*i+25),(math.ceil(200*res[1][i]),25*i),(0,255,0),-1)\n",
    "            cv.putText(image,f\"{res[1][i]*100:>.3f}%\",(210,25*i+25),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "        cv.putText(image,f\"Token {token_list[i]}\",(0,25*i+25),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "    # for i in range(len(token_list)):\n",
    "    #     cv.rectangle(image,(1300,25*i+25),(math.ceil(200*res[0][i])+1300,25*i),(0,255,0),-1)\n",
    "    #     cv.putText(image,f\"Token {token_list[i]}\",(1300,25*i+25),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "    if x[54][0]==0:\n",
    "        cv.putText(image,f\"{sentence[1:]}\",(400,200),cv2.FONT_HERSHEY_SIMPLEX,2,(0,0,255),2)\n",
    "        start_time=0\n",
    "        last_token=\"@\"\n",
    "        return image\n",
    "    if res[1][a[1]]>=ACTIVATE_RATE:\n",
    "        now=datetime.now().timestamp()*1000\n",
    "        if start_time==0 or not last_token==a[1]:\n",
    "            start_time=now\n",
    "            last_token=a[1]\n",
    "        elif now-start_time>1000:\n",
    "            if token_list[a[1]]=='space':\n",
    "                sentence+='_'\n",
    "            else:\n",
    "                sentence+=token_list[a[1]]\n",
    "            start_time=1e18\n",
    "            cv.putText(image,f\"Recognize: {token_list[a[1]]}\",(400,100),cv2.FONT_HERSHEY_SIMPLEX,2,(0,0,255),2)\n",
    "    else:\n",
    "        start_time==0\n",
    "    cv.putText(image,f\"{sentence[1:]}\",(400,200),cv2.FONT_HERSHEY_SIMPLEX,2,(0,0,255),2)\n",
    "    last_rate=res[1][a[1]]\n",
    "    print(token_list[a[0]])\n",
    "    print(token_list[a[1]])\n",
    "    return image\n",
    "model.eval()\n",
    "start_listen(detection)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f94976d7-9f95-4796-a6c3-c13763acbb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]] [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
      "[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]] [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
      "torch.Size([2, 21, 3])\n",
      "0.00219 0.00376 0.00250 0.09469 0.00521 0.02640 0.02697 0.02377 0.14398 0.05386 0.06207 0.04484 0.00922 0.02222 0.00345 0.02066 0.01744 0.04630 0.00320 0.00354 0.17553 0.04375 0.02067 0.02743 0.01144 0.08026 0.00653 0.01813 \n",
      "0.00219 0.00376 0.00250 0.09469 0.00521 0.02640 0.02697 0.02377 0.14398 0.05386 0.06207 0.04484 0.00922 0.02222 0.00345 0.02066 0.01744 0.04630 0.00320 0.00354 0.17553 0.04375 0.02067 0.02743 0.01144 0.08026 0.00653 0.01813 \n"
     ]
    }
   ],
   "source": [
    "start_time=0\n",
    "last_token=\"@\"\n",
    "sentence=\"@\"\n",
    "start_listen(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ebe1bf-89c3-46c1-bf94-8d67fdf2a3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fef88d-886d-479d-a85a-8b08340e5088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
